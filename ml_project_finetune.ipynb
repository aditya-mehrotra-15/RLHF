{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3udX8-3HCfqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4a7a3e-0112-4a80-b07f-39062b22c4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"transformers>=4.35.0\" accelerate bitsandbytes peft trl datasets sentencepiece evaluate rouge_score bert_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 2 — Imports & setup\n",
        "# =====================================\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "1SSutaTiDbEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c040138-0f3a-4af5-9f56-43fa9e5789db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 3 — Define model & tokenizer\n",
        "# =====================================\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525,
          "referenced_widgets": [
            "d483fd2d4b54473fb288b4a2808fd200",
            "1a97a514cdb34a1b8475d68e42f2e3da",
            "ca21266b003e4cf6b1bd4818d200bb79",
            "fc070ef8060f418b80588b67882a0a2b",
            "271487ff744d41c6a8ef0293354574e8",
            "c4ea61494d584ee39f120b46b8baf9cb",
            "081eb2f26d9c4c7d818c586ee0c8c5a5",
            "017623bbaad342eaa0a63bc160fe7b0e",
            "77c137d9ccc44c009ef196083ed0a842",
            "17c37b9237d04d1a85e30166c112e702",
            "49d3933c461d4f528cc188e67e671ac3",
            "cdbcc641e2eb43a690244feeb11c6aa5",
            "f412697c17be47d7b8f4a84b066da8d4",
            "2d20e4f98f00441cacd76a6ceb89fc48",
            "c58a48e7a2ed4bd29e85c3befa132e32",
            "af7f633afbae44f6bba9233ec807950d",
            "38ca9a372bae4bc4987c8065df5ccf3d",
            "e71db16610614c22b70fd0e17a73a294",
            "97237c64cbcd464a8350d45ffaeae89f",
            "0852778480f9494289aa2671f045c326",
            "19f475bf94614cbbb0d0b7c47a7dcfa9",
            "c41b5432a0a743d8b593a93d3cbaf766",
            "578991c882c24069addc89cd5cafcff6",
            "406378ef70874681a8d5f7381edc7194",
            "a2fe5cd81d3f4a3fa0ca9d4a47772275",
            "6d5beae8537747988cd683a7391a9c55",
            "297f32a9b1244f6795bbdf8981874a9e",
            "9d524a28ff1149ccb5b2208cd3470446",
            "cfda77ef0b194dc182883c73794ec28c",
            "6e55286774cc469887628a64d4f96c57",
            "cb89ae48afe543a4813ccfb704af3484",
            "f5561dc90819489ebbc1395b0535acd2",
            "dae91b68a55b4e11953c0a60dca95756",
            "1f43ac641fb34836b2e4085de381d84d",
            "b495ebfee8704cb7ab7dc53961020030",
            "13ba67f27f7643edbe251f72db1ed68f",
            "5b9bb6ddf1fb43afa3f087134ce15d50",
            "7e305df1574c41db9d8bfc5c546a3302",
            "9851f8b126a04e8bbfbc525f248d9635",
            "25078c65d0a248fb8519e0b49cae064f",
            "d4f03f4265e943a28c294ace65b33006",
            "fc314acfcff44ba3bab3d63c7615cc05",
            "e001ca96868144f382d90268f897c691",
            "fb292bf7d2984a2aa6c6cfdb66caf290",
            "a77701235c7b45d58770b5cdb0d05186",
            "973132871354408f859e0cf8e629c42e",
            "a3cd4d80390b4e4ea4d331e1b67e17ec",
            "a4fa752b3fd24de08df9f640990f50d9",
            "2564ace8a4f84e729a0dc15ee3a6d7fd",
            "3521ae0298b541308554c533a8f86e2f",
            "5830e328d4ef4fbd862bad0071ec8125",
            "4c85d705e1634556ae67a8fc8aa25922",
            "4949b8221be94cb29325db69e09c2b1d",
            "d7c5175b39514c969c34221cc7ee26f6",
            "ecaff337a5fe4f60b40562f0afd04502",
            "572967240dc54ab09cfaba7baf00a21b",
            "a688a76b02b64a999f51e9c65b7a76f0",
            "3a9d14c1ba474bb79a777115b1014681",
            "a97d2f8dd48045f7b81503ddd93f9fa2",
            "5f6f2d0aa0dd432ea0788aba27bbecf0",
            "37c0075ca1c0432ca0e6664bb8b65177",
            "c471e503d2514823a0e00f771da04e52",
            "fd5baddcf44d43b6a8d1f9cb8682217d",
            "1d46b074038d4eeaae72dc359e95a9ad",
            "16ff21afc3ef47d4b4e8dc5111ee7a91",
            "fe4133c1af074a36805a9f696a64ca3d",
            "3d1eaa3f9b6a427bade02c28bb475a5f",
            "7426442fac5b4821944b09746e02d78e",
            "770f60bacd614419bc952f07266dc5c0",
            "71bf360150e945198dbf60ee37b2d717",
            "8a842fca2d384610a14afb75f2b7bca0",
            "7ce2515b378a4137ab64b795db02f855",
            "47f7395c38aa498a8703fad26a168044",
            "34760ee0393d499d9989133f1d4af95c",
            "401740a0611b46c18392ca1706f83c43",
            "5303067f729748d4acb7ef4cf8a268c7",
            "3d524883d9fb4a788dddebf093655f44",
            "d75c5951779247a1acdb94d21e4a3cbb",
            "eb4a3a63c499426bb57fddaecc997eed",
            "239f1e688c0544d09e51a8fd2cb08ad1",
            "3e2dc4c279f94b4aa47689ee42f409c6",
            "f08f69f7ff98448c9c45b0b4e417a5c9",
            "bad6a3b45d4140c283ae6c644ffaf4ee",
            "b8a3be4e3aa44a6d9bc2c7a68099f217",
            "e76d24bac2544213b5137bcf0e22c544",
            "2b45a3ef61b045bb9eb0c0d97ccda8fa",
            "8931cfa04bf04a4288d48406d6d30885",
            "735eee1e17084e059a223074cd43511e",
            "0a30e16eef5847faa91b78d8e0bb62e6",
            "aa1692f4d1f84affaf2bd63a986b80fd",
            "df0714e95b164722994285b0a16856b8",
            "cd97be6c7c6e436b9e589fa55eb74302",
            "eb0bab80cd8a414e921fc1445af86084",
            "587dff19616440979101d29109f4352a",
            "ac7c9df213594128aa29da7f12fdae85",
            "987829ca660e43ff86d98a3bee2e697b",
            "c714d13924a842fb880d59760781393e",
            "c229f73f9fda4cf28b1f881c8233b450",
            "b20582cea94147bdb5e296f39791a7be",
            "a1a0d8d943bb4f1bb6a0a5d2364139f7",
            "3ac7ea3833844c4abf8b033c9432e34a",
            "31f995bc172a4e04a638fb96906e6ac1",
            "a1bc3e3ae5ec4e7185412d527b30edf7",
            "e1e553b92ed44168a302ea3662fc4bcd",
            "a222051385ec4f0fb2a1dd51f61ade59",
            "48b1e1b5037042498dd8f0c928054da7",
            "b4313a4bcd3842c299efb9701abdbbdb",
            "c2bcfe652c9b4a4d95a6ce5f69af28d9",
            "df22c63bc85b4ad68374542fe1cc2994",
            "9c2ea08590cb4f4baa85138e8a630730",
            "7f194d46f1d04a0fa2fda52d4a62f413",
            "b3cbb5f90ac24740a6349ea48696175d",
            "464506079e5d4653a5447fb85a1ff784",
            "951acd2edc7f4be383040487d86a93a5",
            "255c9e53b75f4b528ba937ccbdfb5dbb",
            "844a4c29f4b344b6b43170d442fba68b",
            "7642a5781a734865834adf5c5f0b36a4",
            "58a77e48b4744b1b851352d127be4e6b",
            "642efa69592c45cfbf52d9aea0884ea2",
            "e9f29d8594624613abfac898fe538a3d",
            "cce4babd020a4e40864c869ed7809fb6",
            "521dd12938aa40dca5539eb4c87cc680",
            "675937ade7654c26aaf933822ba0cd85",
            "e022fdaed35a4a758ee150d63e67b6c3",
            "a7ff3a0458be4dde9ae5b40a97563465",
            "b9da7c2e9c0f477fa1497f0e319dc98b",
            "a9841d0f854e4e7099c56614d51f40c8",
            "c6d565e247d548339e4bdea137146772",
            "b3b60cc048024c5f99446809ae0b0343",
            "7dc298c7a7194746b734380acc6c0453",
            "758488813ede4af6af499c6e85a5bd57",
            "d9c7afba481c404084d09a6354c06d69"
          ]
        },
        "id": "uAXrPv1ZDkZe",
        "outputId": "acb0e35f-66f4-438e-f013-29b9eb4fd8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d483fd2d4b54473fb288b4a2808fd200"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdbcc641e2eb43a690244feeb11c6aa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "578991c882c24069addc89cd5cafcff6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f43ac641fb34836b2e4085de381d84d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a77701235c7b45d58770b5cdb0d05186"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "572967240dc54ab09cfaba7baf00a21b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d1eaa3f9b6a427bade02c28bb475a5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d75c5951779247a1acdb94d21e4a3cbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a30e16eef5847faa91b78d8e0bb62e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1a0d8d943bb4f1bb6a0a5d2364139f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f194d46f1d04a0fa2fda52d4a62f413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "521dd12938aa40dca5539eb4c87cc680"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 4 — Prepare model for LoRA training\n",
        "# =====================================\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ0z3bB4DvVy",
        "outputId": "02f5e9d6-102b-4598-9804-96c2cf5f4559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 54,525,952 || all params: 7,302,549,504 || trainable%: 0.7467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 5 — Load and preprocess dataset\n",
        "# =====================================\n",
        "dataset = load_dataset(\"coqa\")\n",
        "\n",
        "def format_example(example):\n",
        "    # convert CoQA format -> instruction + answer text\n",
        "    q = example[\"questions\"][-1]\n",
        "    a = example[\"answers\"][\"input_text\"][-1] # Corrected from 'text' to 'input_text'\n",
        "    return {\"text\": f\"Question: {q}\\nAnswer: {a}\"}\n",
        "\n",
        "dataset = dataset.map(format_example)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda e: tokenizer(e[\"text\"], truncation=True, padding=\"max_length\", max_length=512),\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "5e808d38ec4d40f7bde1178d177c8963",
            "86ed87061a1644b7b85c04586b792014",
            "db326a97d0b34e3a85b86f0d8b8902a4",
            "8b4c20080bd649bb81afdd9ebaf93ff8",
            "109d266eb9a9437ca58a85557d7c3050",
            "348d098760584b498507a1a9d70fb6f7",
            "b79976359e7343a5a37cb9013f51e20e",
            "7140d73250284a25abf7de280ccfc7dc",
            "d8f65f1aaa404104b738f3f345d16c05",
            "a0f0d8462fc94aeb8141ee50742b30b4",
            "ac82d447cce14403b414228609b72bca",
            "110de15091344e6383b41925de04feed",
            "f434818d833e46bc9584d6b9678c7cd3",
            "361f7d832f4e4231bab5bd7a54b1fd02",
            "7464057177c8499fa6c9b2ce17831cdd",
            "796731ce9b4949ce9d163804b0adcb74",
            "28cdab57da2a4462bc3f2fa20ab9b737",
            "3574356b5ec9487f9bc5719da382ad9b",
            "c5f0c5417c9648639226e87230d7c031",
            "33579cca669b42f9bf75c4c82ef5df9d",
            "055c2d3ff9f24ab18087156063b8d5a7",
            "66cce446faf54c6c86042ccdd35ed7bb",
            "a5621b36e5de4fc8bfab98d1f34464e4",
            "49b418c70c464fb39c3c9e9c224a95c8",
            "22d6260bb93b4667acd4e81d73b9ee3b",
            "5c21c98d039a4576a4e036ce9482c1f0",
            "403846ef615744c7b669ee3c8d6644a1",
            "e9669bc1eb9d4d4c9cc0bd5cb2fc138f",
            "32db154327f74f99b6c793556d6f1740",
            "bb1612b53ea345488df13ba984e6af2e",
            "1b2e091ff1d641439b7fdfcf121395fc",
            "3bd70a72812e43e48eda6c682f4f3cad",
            "25e340f8d5204ef9a7ca8eb81d83cf69",
            "d4603b8c30224aa2846b4615825c591d",
            "f95d198ee3034eb8a5426dc48cfca59a",
            "9f1c6064e4914cb6b64a4a6f24fa8bb4",
            "532ab7fc620840ac96637e6137959d3d",
            "f43ea511a081401f8fc6f6cead4f6ecd",
            "25f8aa01614d46f88eea75d251ddd1fc",
            "fa34564515124927837e3deb37558d5a",
            "c04e6f05056c415195eedb18f62b0d92",
            "11a6d9f8298e4490ade6175684899be9",
            "660243f2c9264b7eb0de2f2b920864b5",
            "454ea71f9a3b47fc84c9935f864378fa"
          ]
        },
        "id": "KyRZLZCGD1En",
        "outputId": "6a3b0612-8663-4a50-ee41-8462b669a7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7199 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e808d38ec4d40f7bde1178d177c8963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "110de15091344e6383b41925de04feed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7199 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5621b36e5de4fc8bfab98d1f34464e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4603b8c30224aa2846b4615825c591d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['source', 'story', 'questions', 'answers', 'text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 7199\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['source', 'story', 'questions', 'answers', 'text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 6 — Validation loss before fine-tuning (BASE MODEL)\n",
        "# =====================================\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "\n",
        "def eval_loss_and_ppl(model, tokenizer, hf_dataset, batch_size=4, max_length=512):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Remove columns that are not 'input_ids' or 'attention_mask'\n",
        "    hf_dataset_for_collator = hf_dataset.remove_columns([col for col in hf_dataset.column_names if col not in ['input_ids', 'attention_mask']])\n",
        "\n",
        "    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")\n",
        "    loader = DataLoader(hf_dataset_for_collator, batch_size=batch_size, shuffle=False, collate_fn=lambda examples: collator(examples))\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Eval loss\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            tokens = batch[\"input_ids\"].ne(tokenizer.pad_token_id).sum().item()\n",
        "            total_loss += loss.item() * tokens\n",
        "            total_tokens += tokens\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return {\"avg_loss\": avg_loss, \"ppl\": ppl}\n",
        "\n",
        "base_metrics = eval_loss_and_ppl(base_model, tokenizer, tokenized_dataset[\"validation\"].select(range(200)))\n",
        "print(\"Before fine-tuning:\", base_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fd2fdb207f9c4709bf0c43e15a4990f4",
            "56126a19ed98471e88b462e9e03fdd89",
            "2a13755e0bf741afbc024348dee7da36",
            "3fbefa0459d345418f7122381b43bc30",
            "e7bd553010724de98b31c1247ac09a78",
            "c8eb9a1abc0544ef984fc8c7c0358caa",
            "1618350784044b3fa43976b87a27c7fd",
            "602748270e344d56894141f2e31b1250",
            "e7d7e5843da44e7fb8c6b35108879c42",
            "61830344df10493da90e1d26eb2718c9",
            "de4d1b35c6054f25b366968ead645164"
          ]
        },
        "id": "d7A0A0WDD8ky",
        "outputId": "9038a981-638a-4944-c74d-ddfab77b002b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval loss:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd2fdb207f9c4709bf0c43e15a4990f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before fine-tuning: {'avg_loss': 3.7333735501769505, 'ppl': 41.81995216532704}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval helper: loss & perplexity (re-defining is safe)\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def eval_loss_and_ppl(model, tokenizer, hf_dataset, batch_size=4):\n",
        "    model.eval()\n",
        "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # Remove columns that are not 'input_ids' or 'attention_mask'\n",
        "    hf_dataset_for_collator = hf_dataset.remove_columns([col for col in hf_dataset.column_names if col not in ['input_ids', 'attention_mask']])\n",
        "\n",
        "    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")\n",
        "    loader = DataLoader(hf_dataset_for_collator, batch_size=batch_size, shuffle=False, collate_fn=lambda examples: collator(examples))\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Eval loss\"):\n",
        "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            token_count = int(batch.get(\"attention_mask\").sum().item()) if \"attention_mask\" in batch else batch[\"input_ids\"].ne(tokenizer.pad_token_id).sum().item()\n",
        "            total_loss += float(loss.item()) * token_count\n",
        "            total_tokens += token_count\n",
        "    avg_loss = total_loss / max(1, total_tokens)\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return {\"avg_loss\": avg_loss, \"ppl\": ppl}\n"
      ],
      "metadata": {
        "id": "Nf9Z8IjoJRAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from accelerate import Accelerator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Hyperparameters (change as needed) ---\n",
        "num_epochs = 1\n",
        "per_device_train_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "learning_rate = 2e-4\n",
        "train_subset = 800   # set to None to use full train split\n",
        "save_dir = \"./mistral_sft_lora\"\n",
        "\n",
        "# --- Prepare train data loader ---\n",
        "train_data = tokenized_dataset[\"train\"].select(range(min(train_subset, len(tokenized_dataset[\"train\"])))) if train_subset else tokenized_dataset[\"train\"]\n",
        "# Remove columns that are not 'input_ids' or 'attention_mask' for the collator\n",
        "train_data_for_collator = train_data.remove_columns([col for col in train_data.column_names if col not in ['input_ids', 'attention_mask']])\n",
        "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")\n",
        "train_loader = DataLoader(train_data_for_collator, batch_size=per_device_train_batch_size, shuffle=True, collate_fn=lambda exs: collator(exs))\n",
        "\n",
        "# --- Optimizer: only train parameters that require_grad (LoRA params) ---\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "\n",
        "# --- Scheduler ---\n",
        "num_update_steps_per_epoch = max(1, math.ceil(len(train_loader) / gradient_accumulation_steps))\n",
        "max_train_steps = num_epochs * num_update_steps_per_epoch\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=max_train_steps)\n",
        "\n",
        "# --- Accelerator (auto mixed precision) ---\n",
        "if torch.cuda.is_available():\n",
        "    mixed_precision = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
        "else:\n",
        "    mixed_precision = \"no\"\n",
        "\n",
        "accelerator = Accelerator(mixed_precision=mixed_precision)\n",
        "print(\"Accelerator mixed precision:\", mixed_precision)\n",
        "\n",
        "# Prepare objects\n",
        "model, optimizer, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)\n",
        "\n",
        "global_step = 0\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    progress_bar = tqdm(range(len(train_loader)), desc=f\"Epoch {epoch+1}\")\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        with accelerator.accumulate(model):\n",
        "            # ensure batch on device\n",
        "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            # optimizer step handled by accelerator accumulation\n",
        "            if accelerator.sync_gradients:\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        global_step += 1\n",
        "        if (step + 1) % 50 == 0:\n",
        "            try:\n",
        "                print(f\"[Epoch {epoch+1}] step {step+1} loss: {loss.item():.4f}\")\n",
        "            except:\n",
        "                print(f\"[Epoch {epoch+1}] step {step+1} loss: (couldn't read loss scalar)\")\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "# unwrap model (for saving/eval)\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "print(\"Finished training. Global steps:\", global_step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362,
          "referenced_widgets": [
            "3ea08bb91cc14868a3e4b36548c53f63",
            "0b5f4691b6d940d785624d05d899593f",
            "b025f185ba594ea49633cf25f6756b2e",
            "3f1bf84e5e8848a29352e905e5060d88",
            "c0c9836742524e9a9307f5520669121b",
            "dec112a1800c4510b5b5708aae2ebd86",
            "db22b90e10dd42e78259a494f4e84de1",
            "3a8cf2b59ac540c69d557869e1d0c5db",
            "4759c681797a45e68411f22c838ad441",
            "9c815509825d41c5a09889c772bf31d9",
            "18d99a0ef5404e1ea26d8510e272ace7"
          ]
        },
        "id": "8V1CPSd_FPCZ",
        "outputId": "f850d609-cdcd-4568-8645-ddd4476d666c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accelerator mixed precision: bf16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/800 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ea08bb91cc14868a3e4b36548c53f63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] step 50 loss: 3.0450\n",
            "[Epoch 1] step 100 loss: 2.4590\n",
            "[Epoch 1] step 150 loss: 2.4009\n",
            "[Epoch 1] step 200 loss: 2.8688\n",
            "[Epoch 1] step 250 loss: 2.0788\n",
            "[Epoch 1] step 300 loss: 2.5621\n",
            "[Epoch 1] step 350 loss: 2.0653\n",
            "[Epoch 1] step 400 loss: 2.2547\n",
            "[Epoch 1] step 450 loss: 2.9567\n",
            "[Epoch 1] step 500 loss: 1.4165\n",
            "[Epoch 1] step 550 loss: 2.5119\n",
            "[Epoch 1] step 600 loss: 2.9169\n",
            "[Epoch 1] step 650 loss: 2.2854\n",
            "[Epoch 1] step 700 loss: 2.3622\n",
            "[Epoch 1] step 750 loss: 2.5478\n",
            "[Epoch 1] step 800 loss: 2.5076\n",
            "Finished training. Global steps: 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save PEFT adapter + tokenizer\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "# `unwrapped_model` is PeftModel wrapping the base; save_pretrained writes the adapter\n",
        "try:\n",
        "    unwrapped_model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(\"Saved LoRA adapter + tokenizer to:\", save_dir)\n",
        "except Exception as e:\n",
        "    # fallback if save_pretrained fails on this wrapper\n",
        "    print(\"Primary save failed:\", e)\n",
        "    try:\n",
        "        # If `unwrapped_model` is PeftModel, this should work\n",
        "        from peft import PeftModel\n",
        "        if isinstance(unwrapped_model, PeftModel):\n",
        "            unwrapped_model.save_pretrained(save_dir)\n",
        "            tokenizer.save_pretrained(save_dir)\n",
        "            print(\"Saved via PeftModel fallback to:\", save_dir)\n",
        "    except Exception as e2:\n",
        "        print(\"Fallback save also failed:\", e2)\n"
      ],
      "metadata": {
        "id": "TL_ujUejFROf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850c5209-0a50-4279-caca-a2fee136bb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LoRA adapter + tokenizer to: ./mistral_sft_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load adapter onto a fresh base model (keeps base_model unchanged)\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        ")\n",
        "\n",
        "# load base in 4-bit again (device_map=\"auto\")\n",
        "base_for_eval = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_cfg, device_map=\"auto\")\n",
        "base_for_eval.config.use_cache = False\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_for_eval, save_dir)\n",
        "ft_model.eval()\n",
        "print(\"Loaded fine-tuned PEFT model from\", save_dir)\n"
      ],
      "metadata": {
        "id": "TypAaLj8FTjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a535d1fbf1a74b8da6de37e1061d9333",
            "81c2ce444329486e99e5a3ace32a3c44",
            "91afaf75c8e04afdbac48640234c5e0d",
            "313ee9e6ed6f41d28a43d2ad691f5eab",
            "07a7ce8434d24ba89dce26f36364c434",
            "2aca9a53b43f4871b087c172701cd6fe",
            "f57dda2ff6fc42fe989dd815eec9a4e2",
            "9f53894b9e8545c3a000c1007e9f65b7",
            "179c781988f3437285ec5c631637fb6c",
            "fbeb7bb0af2e4c4e936369e908be46d7",
            "cd9c71b9832f4d8d919a6c7d4c770d54"
          ]
        },
        "outputId": "ac259549-1d96-4821-ae8d-2b1655014847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a535d1fbf1a74b8da6de37e1061d9333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded fine-tuned PEFT model from ./mistral_sft_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small subset for speed; pick a larger slice for final evaluation\n",
        "val_sample = tokenized_dataset[\"validation\"].select(range(min(200, len(tokenized_dataset[\"validation\"]))))\n",
        "ft_metrics = eval_loss_and_ppl(ft_model, tokenizer, val_sample, batch_size=2)\n",
        "print(\"Fine-tuned model metrics (subset):\", ft_metrics)\n",
        "\n",
        "# if you saved base metrics earlier, compare them here:\n",
        "try:\n",
        "    print(\"Previously computed base metrics:\", base_metrics)\n",
        "except NameError:\n",
        "    print(\"No in-memory base_metrics found (you can re-run base eval if needed).\")\n"
      ],
      "metadata": {
        "id": "PQ9wP0l-LPUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "02b1d6dfbc814a8fa82d04abad56df90",
            "8e669a5fb45b4862b47ad43d38f25a4b",
            "33ec861c19ff4e0cbfdaa13e11b4b172",
            "0ccdf409bf854364911323b40684da85",
            "c3c6b3f3e4c249c988e1db93251f3f2b",
            "43fcbc5e5cae489096bd822eb328c106",
            "1b3a4c348ef942cead318887d042dcfb",
            "f6788485d9274ad1b3de3960648037b2",
            "7c4df92cb4e84cb8a42c4cc7b1a77d1b",
            "eaa0e3d65d8a40ab970b198339e0ad84",
            "76c6fedd21b8466297fa30171bf6ef30"
          ]
        },
        "outputId": "30ec0664-26d2-408a-a4b2-d0f549e58ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval loss:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02b1d6dfbc814a8fa82d04abad56df90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model metrics (subset): {'avg_loss': 2.398970152808463, 'ppl': 11.01183003690711}\n",
            "Previously computed base metrics: {'avg_loss': 3.7333735501769505, 'ppl': 41.81995216532704}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you still have the original `base_model`, use it; otherwise re-load base similarly as above.\n",
        "def generate_responses(model, tokenizer, prompts, max_new_tokens=64, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    outs = []\n",
        "    for p in prompts:\n",
        "        inputs = tokenizer(p, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "        outs.append(tokenizer.decode(gen[0], skip_special_tokens=True))\n",
        "    return outs\n",
        "\n",
        "# Build small prompt list from your raw dataset (or use pre-made prompts)\n",
        "prompts = [ex[\"text\"].split(\"\\n\")[0] for ex in tokenized_dataset[\"validation\"].select(range(10))]\n",
        "\n",
        "# Use base_for_eval, which is a freshly loaded base model instance\n",
        "base_preds = generate_responses(base_for_eval, tokenizer, prompts)\n",
        "ft_preds = generate_responses(ft_model, tokenizer, prompts)\n",
        "\n",
        "for i, p in enumerate(prompts):\n",
        "    print(f\"\\nPROMPT: {p}\")\n",
        "    print(\"BASE : \", base_preds[i])\n",
        "    print(\"FINE : \", ft_preds[i])\n"
      ],
      "metadata": {
        "id": "r6yqF5xrSUsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c67b431-25b3-4fe1-9e8d-c758a6538e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Question: Did they want Cotton to change the color of her fur?\n",
            "BASE :  Question: Did they want Cotton to change the color of her fur?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her personality?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her name?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her age?\n",
            "Answer: No.\n",
            "\n",
            "FINE :  Question: Did they want Cotton to change the color of her fur?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her personality?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her name?\n",
            "Answer: No.\n",
            "Question: Did they want Cotton to change her age?\n",
            "Answer: No.\n",
            "\n",
            "\n",
            "PROMPT: Question: were they excited\n",
            "BASE :  Question: were they excited?\n",
            "Answer: yes.\n",
            "Question: what did they do?\n",
            "Answer: they went to the beach.\n",
            "Question: what did they do there?\n",
            "Answer: they played in the sand.\n",
            "Question: what did they do after that?\n",
            "Answer: they went home.\n",
            "Question\n",
            "FINE :  Question: were they excited?\n",
            "Answer: yes.\n",
            "Question: what did they do?\n",
            "Answer: they went to the beach.\n",
            "Question: what did they do there?\n",
            "Answer: they played in the sand.\n",
            "Question: what did they do after that?\n",
            "Answer: they went home.\n",
            "Question\n",
            "\n",
            "PROMPT: Question: What is the first phrase I learn?\n",
            "BASE :  Question: What is the first phrase I learn?\n",
            "Answer: \"I'm a little teapot.\"\n",
            "Question: What is the second phrase I learn?\n",
            "Answer: \"Short and stout.\"\n",
            "Question: What is the third phrase I learn?\n",
            "Answer: \"Here is my handle, here is my spout.\"\n",
            "Question\n",
            "FINE :  Question: What is the first phrase I learn?\n",
            "Answer: \"I'm a little teapot.\"\n",
            "Question: What is the second phrase I learn?\n",
            "Answer: \"Short and stout.\"\n",
            "Question: What is the third phrase I learn?\n",
            "Answer: \"Here is my handle, here is my spout.\"\n",
            "Question\n",
            "\n",
            "PROMPT: Question: What had he been before?\n",
            "BASE :  Question: What had he been before?\n",
            "Answer: a teacher.\n",
            "Question: What was he doing now?\n",
            "Answer: working as a waiter.\n",
            "Question: What was his name?\n",
            "Answer: Mr. Porter.\n",
            "Question: What was his wife's name?\n",
            "Answer: Mrs. Porter.\n",
            "\n",
            "FINE :  Question: What had he been before?\n",
            "Answer: a teacher.\n",
            "Question: What was he doing now?\n",
            "Answer: working as a waiter.\n",
            "Question: What was his name?\n",
            "Answer: Mr. Porter.\n",
            "Question: What was his wife's name?\n",
            "Answer: Mrs. Porter.\n",
            "\n",
            "\n",
            "PROMPT: Question: When would he be back?\n",
            "BASE :  Question: When would he be back?\n",
            "Answer: in a few days.\n",
            "Question: What would he do then?\n",
            "Answer: he would go to the hospital.\n",
            "Question: What would he do there?\n",
            "Answer: he would get a new heart.\n",
            "Question: What would he do after that?\n",
            "Answer: he\n",
            "FINE :  Question: When would he be back?\n",
            "Answer: in a few days.\n",
            "Question: What would he do then?\n",
            "Answer: he would go to the hospital.\n",
            "Question: What would he do there?\n",
            "Answer: he would get a new heart.\n",
            "Question: What would he do after that?\n",
            "Answer: he\n",
            "\n",
            "PROMPT: Question: which neighborhoods?\n",
            "BASE :  Question: which neighborhoods?\n",
            "Answer: the ones that are not in the city.\n",
            "Question: what is the name of the city?\n",
            "Answer: the city of New York.\n",
            "Question: what is the name of the neighborhoods?\n",
            "Answer: the neighborhoods of Brooklyn and Queens.\n",
            "Question: what is the\n",
            "FINE :  Question: which neighborhoods?\n",
            "Answer: the ones that are not in the city.\n",
            "Question: what is the name of the city?\n",
            "Answer: the city of New York.\n",
            "Question: what is the name of the neighborhoods?\n",
            "Answer: the neighborhoods of Brooklyn and Queens.\n",
            "Question: what is the\n",
            "\n",
            "PROMPT: Question: What guided RJ home?\n",
            "BASE :  Question: What guided RJ home?\n",
            "Answer: the light from the lighthouse.\n",
            "Question: What did RJ do when he got home?\n",
            "Answer: he went to bed.\n",
            "Question: What did RJ do when he woke up?\n",
            "Answer: he went to the lighthouse.\n",
            "Question: What did\n",
            "FINE :  Question: What guided RJ home?\n",
            "Answer: the light from the lighthouse.\n",
            "Question: What did RJ do when he got home?\n",
            "Answer: he went to bed.\n",
            "Question: What did RJ do when he woke up?\n",
            "Answer: he went to the lighthouse.\n",
            "Question: What did\n",
            "\n",
            "PROMPT: Question: How many were snorkeling?\n",
            "BASE :  Question: How many were snorkeling?\n",
            "Answer: 2.\n",
            "Question: How many were swimming?\n",
            "Answer: 1.\n",
            "Question: How many were floating?\n",
            "Answer: 1.\n",
            "Question: How many were diving?\n",
            "Answer: 1.\n",
            "Question: How many were on the boat?\n",
            "\n",
            "FINE :  Question: How many were snorkeling?\n",
            "Answer: 2.\n",
            "Question: How many were swimming?\n",
            "Answer: 1.\n",
            "Question: How many were floating?\n",
            "Answer: 1.\n",
            "Question: How many were diving?\n",
            "Answer: 1.\n",
            "Question: How many were on the boat?\n",
            "\n",
            "\n",
            "PROMPT: Question: How did his body react to the tea?\n",
            "BASE :  Question: How did his body react to the tea?\n",
            "Answer: He felt a little better.\n",
            "Question: What did he do next?\n",
            "Answer: He went to the bathroom.\n",
            "Question: What did he do in the bathroom?\n",
            "Answer: He threw up.\n",
            "Question: What did he do after he threw up?\n",
            "Answer:\n",
            "FINE :  Question: How did his body react to the tea?\n",
            "Answer: He felt a little better.\n",
            "Question: What did he do next?\n",
            "Answer: He went to the bathroom.\n",
            "Question: What did he do in the bathroom?\n",
            "Answer: He threw up.\n",
            "Question: What did he do after he threw up?\n",
            "Answer:\n",
            "\n",
            "PROMPT: Question: What is Holmes being charged with?\n",
            "BASE :  Question: What is Holmes being charged with?\n",
            "Answer: murder.\n",
            "Question: Who is the victim?\n",
            "Answer: his wife.\n",
            "Question: Who is the suspect?\n",
            "Answer: Holmes.\n",
            "Question: Who is the detective?\n",
            "Answer: Holmes.\n",
            "Question: Who is the prosecutor?\n",
            "Answer: Holmes.\n",
            "FINE :  Question: What is Holmes being charged with?\n",
            "Answer: murder.\n",
            "Question: Who is the victim?\n",
            "Answer: his wife.\n",
            "Question: Who is the suspect?\n",
            "Answer: Holmes.\n",
            "Question: Who is the detective?\n",
            "Answer: Holmes.\n",
            "Question: Who is the prosecutor?\n",
            "Answer: Holmes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute BLEU/ROUGE/BERTScore and a paired bootstrap CI for BLEU\n",
        "import evaluate\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "refs = [ex[\"text\"] for ex in tokenized_dataset[\"validation\"].select(range(10))]  # adjust selection to match prompts size\n",
        "\n",
        "def compute_generation_metrics(references, predictions):\n",
        "    res = {}\n",
        "    res[\"bleu\"] = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
        "    r = rouge.compute(predictions=predictions, references=references)\n",
        "    res[\"rougeL\"] = r.get(\"rougeL\", None)\n",
        "    bs = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "    res[\"bertscore_f1\"] = float(np.mean(bs[\"f1\"]))\n",
        "    return res\n",
        "\n",
        "metrics_base = compute_generation_metrics(refs, base_preds)\n",
        "metrics_ft = compute_generation_metrics(refs, ft_preds)\n",
        "print(\"Base metrics:\", metrics_base)\n",
        "print(\"FT metrics  :\", metrics_ft)\n",
        "\n",
        "# Paired bootstrap for BLEU diff\n",
        "def paired_bootstrap_metric_diff(references, preds_a, preds_b, metric_fn, n_bootstrap=1000, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    n = len(references)\n",
        "    diffs = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        idxs = [rng.randrange(n) for _ in range(n)]\n",
        "        refs_s = [references[i] for i in idxs]\n",
        "        a_s = [preds_a[i] for i in idxs]\n",
        "        b_s = [preds_b[i] for i in idxs]\n",
        "        val_a = metric_fn(refs_s, a_s)\n",
        "        val_b = metric_fn(refs_s, b_s)\n",
        "        diffs.append(val_b - val_a)\n",
        "    diffs = np.array(diffs)\n",
        "    return float(np.mean(diffs)), (float(np.percentile(diffs, 2.5)), float(np.percentile(diffs, 97.5)))\n",
        "\n",
        "def bleu_metric_fn(refs, preds):\n",
        "    return bleu.compute(predictions=preds, references=refs)[\"bleu\"]\n",
        "\n",
        "mean_diff, ci = paired_bootstrap_metric_diff(refs, base_preds, ft_preds, bleu_metric_fn, n_bootstrap=500)\n",
        "print(\"BLEU mean diff (FT - BASE):\", mean_diff, \"95% CI:\", ci)\n"
      ],
      "metadata": {
        "id": "24XgUmwGSVUA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "14fe3645e6b348c8bff65ee3203f4421",
            "290f6c143399412ab097f03210ad1073",
            "c09a3b5152054f23a7d694a30be65c72",
            "418c2273b6034a7cafc83c7278c33e27",
            "e80a905dac4c4f76a4e8b6ff1afa8cc5",
            "4ac8f496ceb84a6d81e2d4b48aab0db4",
            "a300525ac8b74d25969172b8eb038f4b",
            "89e5217b5e514ec5be067e86077a7fb1",
            "fd262c8eea474c309dab63728da741f2",
            "ab41c6b6edfd4c2fba148d2365625dd6",
            "013a29ea57374f94ac6f989d5404f552",
            "86272cf27cfb4b82a344b48f0c937f63",
            "a26e22716c4445d8b03cd33a18808ea8",
            "f3b071fca721452c97324c8a026e93dc",
            "a0712f2997a949bba72b5ef68f77872c",
            "8dc9e6c93cb146cbb54d211cf86e62c9",
            "3bbade3e457e46deb40a0d9e2cd6e662",
            "870f8e5d8469452ca6dd5423cd45d934",
            "a09a6dfaa820405dbfeb503c1f862e34",
            "20b7e3486dcb4b74a51407a145dba3de",
            "6dba6ec84c404cecb9420a9ce597d2a1",
            "b0dbbf48cb644acf8147de9419ec6e9d",
            "6bdd3dae3e474d53842d8d5eba8ca0b7",
            "ee9645e18d2e4f6b9baba41a9b0e070f",
            "2a6860742f034b94891366fb3a31cefe",
            "33c743fdb18e4cf9b61688aec0d0110e",
            "08bac36f1e184691a4f9974bbcabb993",
            "0a9f14aa7fb44f6ba24b006a76d816c8",
            "24843c04fd8b4e3d858330f55a8613c5",
            "ed365e8559e14e05b842515d8e074315",
            "875daa3b1b044f249e939aa5e41eb726",
            "e451fc732a66449ca2f6cf07ca506a62",
            "13a65b7f525f4517935eb0c3d14a85ae",
            "c6863ca56b7f4abf8a9e08d2d90feb33",
            "f292e256287f4ea1a5062c5a59edef49",
            "1c6b1e9bd3f841f5b57b4d9e2e23064d",
            "78f754e7d17f4fdeba42caac781e3137",
            "15d6423cafd84fb8a86ec882d73db17f",
            "fcae28adca184dd9b17e80ab2db5889f",
            "caa335e8a38e4e1db49c52c913d306f4",
            "c30bb7382f0f47ce820e952bf12d7dde",
            "30d454e9dff24803aff78833d0c0108a",
            "89f97492738242a4a713fadaefcc8fd7",
            "c434c550f7184df9bdf9da858e9dd981",
            "12df4d207f224bbd8de040ca23fc43a5",
            "8c5925ac863e4a53996dea144d4d16cc",
            "edab525a16874674984f1aa51011147e",
            "b9117236848f4b27b7a5431635641d6e",
            "ca1cd3fa05df403799435b9efd552d39",
            "221a690791bb4a6db07b0cd267257935",
            "bc297a2ec17b451498ff27c9bd15e441",
            "d2dbcc14fafc4e63a0049f3742c7dea9",
            "f26ed2020c4a42559cc9211673d70b17",
            "121de830a7504d948d8fb181132ff6ec",
            "5ed6cae85f07445ea40f72cb41be43d4",
            "0e85e468213e41a5beaddea5304847ff",
            "5e5a5eb842d949e1836b4329a0fa9447",
            "f46cc6226cfe42c0bcb7b60de0932e25",
            "9ca1396d17ed4347abfe4d5fffc7479f",
            "314f64eba7604c26ad8e321cd7114e67",
            "20ca8ca240444578acf8700040a521fc",
            "974da221ba294f06a343e0c3e9309375",
            "bee58e76d8fa455b99c3ed557c309820",
            "bd24cdfa29024fafb7ee0828d1972571",
            "2abbc0acfd9847a09f237c70b3a374d6",
            "0c00f91f790f49c0869e158e5a54f4f0"
          ]
        },
        "outputId": "a7ac3ecc-d24c-4c14-8636-df893b3549e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14fe3645e6b348c8bff65ee3203f4421"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86272cf27cfb4b82a344b48f0c937f63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bdd3dae3e474d53842d8d5eba8ca0b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6863ca56b7f4abf8a9e08d2d90feb33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12df4d207f224bbd8de040ca23fc43a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e85e468213e41a5beaddea5304847ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base metrics: {'bleu': 0.15910334818940228, 'rougeL': np.float64(0.30377361536623115), 'bertscore_f1': 0.8966879725456238}\n",
            "FT metrics  : {'bleu': 0.15910334818940228, 'rougeL': np.float64(0.30377361536623115), 'bertscore_f1': 0.8966879725456238}\n",
            "BLEU mean diff (FT - BASE): 0.0 95% CI: (0.0, 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xpT9lhPjU15E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}